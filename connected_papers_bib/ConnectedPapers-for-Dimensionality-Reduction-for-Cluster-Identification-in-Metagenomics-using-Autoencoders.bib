@article{f41d5e6a7338f47f1f271b11c6080f33b9b8fdc6,
title = {Dimensionality Reduction for Cluster Identification in Metagenomics using Autoencoders},
year = {2020},
url = {https://www.semanticscholar.org/paper/f41d5e6a7338f47f1f271b11c6080f33b9b8fdc6},
abstract = {Metagenomics is the study of the genomic content of the microbial organisms extracted from a sample in their natural habitats. These unknown collections of genomic data are analyzed without any prior lab-based cultivation to avoid amplification bias. One of the vital aspects of metagenomics analysis is the visualization of the information that is derived from the genomic sequences of a microbiome sample. In a successful visualization, the congruent reads of the sequences should appear in clusters depending on the diversity and taxonomy of the microorganisms in the sequenced sample. In converting higher dimensional sequence data into lower dimensional data for visualization purposes, preserving the genomic characteristics is given the highest priority. In this process, the demand for precise and efficient methods of dimensionality reduction is crucial. Currently, Principle Component Analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE) are used for dimensionality reduction purposes in metagenomics, which are linear and nonlinear techniques respectively. Although the above-mentioned techniques are widely used, there are shortcomings in accuracy and efficiency in terms of visualizations. In this paper, we explore the possibility of using autoencoders, a deep learning technique, to get a rich dimensionality reduction, overcoming the prevailing impediments of PCA and t-SNE and outperforming them to achieve better metagenomic visualizations.},
author = {U. Maduranga and K. Wijegunarathna and Sadeep Weerasinghe and I. Perera and A. Wickramarachchi},
doi = {10.1109/ICTer51097.2020.9325447},
}

@article{66c33452b5b41375a0865ba45a2c237ae1371d3c,
title = {Type 1 and 2 mixtures of divergences for stoc hastic neighbor embedding},
year = {2012},
url = {https://www.semanticscholar.org/paper/66c33452b5b41375a0865ba45a2c237ae1371d3c},
abstract = {Stochastic neighbor embedding (SNE) is a method of di- mensionality reduction (DR) that involves softmax similarities measured between all pairs of data points. In order to build a low-dimensional embedding, SNE tries to reproduce the similarities observed in the high- dimensional data space. The capability of softmax similarities to fight the phenomenon of norm concentration has been studied in previous work. This paper investigates a complementary aspect, namely, the cost function that quantifies the mismatch between the high- and low-dimensional sim- ilarities. We show experimentally that switching from a simple Kullback- Leibler divergences to mixtures of dual divergences increases the quality of DR. This modification brings SNE to the performance level of its Student t-distributed variant, without the need to resort to non-identical similarity definitions in the high- and low-dimensional spaces. These results allow us to conclude that future improvements in similarity-based DR will likely emerge from better definitions of the cost function.},
author = {J. Lee},
}

@article{510ae741c9873341567b1556f78add0d77454b86,
title = {PixelSNE: Visualizing Fast with Just Enough Precision via Pixel-Aligned Stochastic Neighbor Embedding},
year = {2016},
url = {https://www.semanticscholar.org/paper/510ae741c9873341567b1556f78add0d77454b86},
abstract = {Embedding and visualizing large-scale high-dimensional data in a two-dimensional space is an important problem since such visualization can reveal deep insights out of complex data. Most of the existing embedding approaches, however, run on an excessively high precision, ignoring the fact that at the end, embedding outputs are converted into coarse-grained discrete pixel coordinates in a screen space. Motivated by such an observation and directly considering pixel coordinates in an embedding optimization process, we accelerate Barnes-Hut tree-based t-distributed stochastic neighbor embedding (BH-SNE), known as a state-of-the-art 2D embedding method, and propose a novel method called PixelSNE, a highly-efficient, screen resolution-driven 2D embedding method with a linear computational complexity in terms of the number of data items. Our experimental results show the significantly fast running time of PixelSNE by a large margin against BH-SNE, while maintaining the minimal degradation in the embedding quality. Finally, the source code of our method is publicly available at this https URL},
author = {Minjeong Kim and Min-Je Choi and Sunwoong Lee and Jian Tang and Haesun Park and J. Choo},
arxivid = {1611.02568},
}

@article{a170cb44823b6cba96ef11b4b9fcbfa6b8037d82,
title = {Cluster Identification in Metagenomics – A Novel Technique of Dimensionality Reduction through Autoencoders},
year = {2021},
url = {https://www.semanticscholar.org/paper/a170cb44823b6cba96ef11b4b9fcbfa6b8037d82},
abstract = {Analysis of metagenomic data is not only challenging because they are acquired from a sample in their natural habitats but also because of the high volume and high dimensionality. The fact that no prior lab based cultivation is carried out in metagenomics makes the inference on the presence of numerous microorganisms all the more challenging, accentuating the need for an informative visualization of this data. In a successful visualization, the congruent reads of the sequences should appear in clusters depending on the diversity and taxonomy of the microorganisms in the sequenced sample. The metagenomic data represented by their oligonucleotide frequency vectors is inherently high dimensional and therefore impossible to visualize as is. This raises the need for a dimensionality reduction technique to convert these higher dimensional sequence data into lower dimensional data for visualization purposes. In this process, preservation of the genomic characteristics must be given highest priority. Currently, for dimensionality reduction purposes in metagenomics, Principal Component Analysis (PCA) which is a linear technique and t-distributed Stochastic Neighbor Embedding (t-SNE), a non-linear technique, are widely used. Albeit their wide use, these techniques are not exceptionally suited to the domain of metagenomics with certain shortcomings and weaknesses. Our research explores the possibility of using autoencoders, a deep learning technique, that has the potential to overcome the prevailing impediments of the existing dimensionality reduction techniques eventually leading to richer visualizations.},
author = {K. Wijegunarathna and U. Maduranga and Sadeep Weerasinghe and I. Perera and Anuradha Wickaramarachchi},
doi = {10.4038/ICTER.V14I2.7224},
}

@article{28c303de856320152f746b0f5bbeba818932c04b,
title = {MetaG: a comprehensive visualization tool to explore metagenomes},
year = {2020},
url = {https://www.semanticscholar.org/paper/28c303de856320152f746b0f5bbeba818932c04b},
abstract = {Sequencing in metagenomes opens new ways of analyzing genomics in microbial communities in their habitats. Analyzing metagenomes has been a challenge not only because they are acquired from unknown collections without any prior lab-based cultivation, but also the volume. The value of these metagenomic data can be greatly enhanced by integrating with an informative and interactive visualization tool. Need for such tools is growing but still insufficient in number. MetaG, a standalone metagenomic analysis tool which converts higher dimensional sequence data into lower dimensional visualizations through deep learning techniques, is uniquely designed to produce rich lower dimensional representations of oligonucleotide frequency vectors of the population-level genomic diversity of the microbial organisms. Users can opt their preference in dimensionality reduction, between Principal Component Analysis (PCA), t- distributed Stochastic Neighbor Embedding (t-SNE) and autoencoders. Additionally, MetaG provides information on the presence, diversity and abundance of organisms by analyzing alignments against a database that contains taxonomy information. It caters more flexibility with number of visualizations, providing insights of complex microbial communities. MetaG, written in Python is free and open-source with the code publicly accessible.},
author = {K. Wijegunarathna and U. Maduranga and Sadeep Weerasinghe and I. Perera and A. Wickramarachchi},
doi = {10.1109/BIBM49941.2020.9313166},
}

@article{d3c0c6267058bd796c824b4fe46722066f1c587f,
title = {Sensitivity to parameter and data variations in dimensionality reduction techniques},
year = {2013},
url = {https://www.semanticscholar.org/paper/d3c0c6267058bd796c824b4fe46722066f1c587f},
abstract = {Dimensionality reduction techniques aim at representing high dimensional data in a meaningful and lower-dimensional space, improving the human comprehension and interpretation of data. In recent years, newer nonlinear techniques have been proposed in order to address the limitation of linear techniques. This paper presents a study of the stability of some of these dimensionality reduction techniques, analyzing their behavior under changes in the parameters and the data. The performances of these techniques are investigated on artificial datasets. The paper presents these results by identifying the weaknesses of each technique, and suggests some data-processing tasks to improve the stability.},
author = {Francisco J. García-Fernández and M. Verleysen and J. Lee and Ignacio Díaz Blanco},
}

@article{eca48d48b9fcc5417d945f30b791d716a09d4d6d,
title = {Unsupervised dimensionality reduction: Overview and recent advances},
year = {2010},
url = {https://www.semanticscholar.org/paper/eca48d48b9fcc5417d945f30b791d716a09d4d6d},
abstract = {Unsupervised dimensionality reduction aims at representing high-dimensional data in lower-dimensional spaces in a faithful way. Dimensionality reduction can be used for compression or denoising purposes, but data visualization remains one its most prominent applications. This paper attempts to give a broad overview of the domain. Past develoments are briefly introduced and pinned up on the time line of the last eleven decades. Next, the principles and techniques involved in the major methods are described. A taxonomy of the methods is suggested, taking into account various properties. Finally, the issue of quality assessment is briefly dealt with.},
author = {J. Lee and M. Verleysen},
doi = {10.1109/IJCNN.2010.5596721},
}

@article{598dac19d3cd53fa302100ac1c85caa1472fb693,
title = {Unsupervised fuzzy binning of metagenomic sequence fragments on three-dimensional Barnes-Hut t-Stochastic Neighbor Embeddings},
year = {2018},
url = {https://www.semanticscholar.org/paper/598dac19d3cd53fa302100ac1c85caa1472fb693},
abstract = {Shotgun metagenomic studies attempt to reconstruct population genome sequences from complex microbial communities. In some traditional genome demarcation approaches, high-dimensional sequence data are embedded into two-dimensional spaces and subsequently binned into candidate genomic populations. One such approach uses a combination of the Barnes-Hut approximation and the $t -$Stochastic Neighbor Embedding (BH-SNE) algorithm for dimensionality reduction of DNA sequence data pentamer profiles; and demarcation of groups based on Gaussian mixture models within humanimposed boundaries. We found that genome demarcation from three-dimensional BH-SNE embeddings consistently results in more accurate binnings than 2-D embeddings. We further addressed the lack of a priori population number information by developing an unsupervised binning approach based on the Subtractive and Fuzzy c-means (FCM) clustering algorithms combined with internal clustering validity indices. Lastly, we addressed the subject of shared membership of individual data objects in a mixed community by assigning a degree of membership to individual objects using the FCM algorithm, and discriminated between confidently binned and uncertain sequence data objects from the community for subsequent biological interpretation. The binning of metagenome sequence fragments according to thresholds in the degree of membership opens the door for the identification of horizontally transferred elements and other genomic regions of uncertain assignment in which biologically meaningful information resides. The reported approach improves the unsupervised genome demarcation of populations within complex communities, increases the confidence in the coherence of the binned elements, and enables the identification of evolutionary processes ignored in hard-binning approaches in shotgun metagenomic studies.},
author = {L. Ariza-Jiménez and O. Quintero and N. Pinel},
doi = {10.1109/EMBC.2018.8512529},
pmid = {30440633},
}

@article{b1a1fbf2ffb922fbfc6c4d0565925497a82d6b85,
title = {Parametric nonlinear dimensionality reduction using kernel t-SNE},
year = {2015},
url = {https://www.semanticscholar.org/paper/b1a1fbf2ffb922fbfc6c4d0565925497a82d6b85},
abstract = {Abstract Novel non-parametric dimensionality reduction techniques such as t-distributed stochastic neighbor embedding (t-SNE) lead to a powerful and flexible visualization of high-dimensional data. One drawback of non-parametric techniques is their lack of an explicit out-of-sample extension. In this contribution, we propose an efficient extension of t-SNE to a parametric framework, kernel t-SNE, which preserves the flexibility of basic t-SNE, but enables explicit out-of-sample extensions. We test the ability of kernel t-SNE in comparison to standard t-SNE for benchmark data sets, in particular addressing the generalization ability of the mapping for novel data. In the context of large data sets, this procedure enables us to train a mapping for a fixed size subset only, mapping all data afterwards in linear time. We demonstrate that this technique yields satisfactory results also for large data sets provided missing information due to the small size of the subset is accounted for by auxiliary information such as class labels, which can be integrated into kernel t-SNE based on the Fisher information.},
author = {A. Gisbrecht and Alexander Schulz and B. Hammer},
doi = {10.1016/j.neucom.2013.11.045},
}

@article{ac5102b1ab1ec129c37b0e282a5405055c379d97,
title = {An Experimental Study of Dimensionality Reduction Methods},
year = {2017},
url = {https://www.semanticscholar.org/paper/ac5102b1ab1ec129c37b0e282a5405055c379d97},
abstract = {Dimensionality reduction (DR) lowers the dimensionality of a high-dimensional data set by reducing the number of features for each pattern. The importance of DR techniques for data analysis and visualization led to the development of a large diversity of DR methods. The lack of comprehensive comparative studies makes it difficult to choose the best DR methods for a particular task based on known strengths and weaknesses. To close the gap, this paper presents an extensive experimental study comparing 29 DR methods on 13 artificial and real-world data sets. The performance assessment of the study is based on six quantitative metrics. According to our benchmark and evaluation scheme, the methods mMDS, GPLVM, and PCA turn out to outperform their competitors, although exceptions are revealed for special cases.},
author = {Almuth Meier and Oliver Kramer},
doi = {10.1007/978-3-319-67190-1_14},
}

@article{2fcae211b7ae59e1b337b81b0542e29e1360429c,
title = {Visualization of High Dimensional Models within the SG++ Data Mining Pipeline},
year = {2019},
url = {https://www.semanticscholar.org/paper/2fcae211b7ae59e1b337b81b0542e29e1360429c},
abstract = {Visualization is an essential aspect during a knowledge discovery process, since it provides the user with information which cannot be easily identified just by looking at raw numbers. The SG++ Datamining Pipeline is a component of the SG++ Toolbox used to generate such data mining models by utilizing sparse grids methods. Until recently, there was no way the end user could visualize these models and therefore a new visualization module was implemented. In this report, a description of these new implemented module will be given, explaining the approach taken when handling data on higher dimensions. Additionally, some visual results will be shown for density estimation and classification models for high dimensional data sets.},
author = {V. Anguiano},
}

@article{555101f571c928026daaf91d97481da5ea7e2eb5,
title = {A supervised learning approach for detecting erroneous samples in embeddings},
year = {2020},
url = {https://www.semanticscholar.org/paper/555101f571c928026daaf91d97481da5ea7e2eb5},
abstract = {Visualizing multidimensional data has been a crucial task in recent years regarding the growing amount of data from various sources. To achieve this, dimensionality reduction algorithms have been used to reduce the number of dimensions for visualization of the data on a screen. However, these algorithms may fail to faithfully represent high dimensional data in lower dimensions and eventually lead to erroneous visualizations. In this work, we propose an error detection algorithm for dimensionality reduction algorithms based on recently developed error prediction algorithms for medical image registration. The proposed algorithm matches the neighborhoods of high and low dimensional data with different similarity measures and predicts the errors using a random forest classifier. The results on three datasets show that the proposed algorithm can successfully detect errors with an accuracy up to 86% and area under the curve score of 0.81 .},
author = {Gorkem Saygili},
doi = {10.3906/elk-1909-162},
}

@article{7ccfaae1079ee90208ecfb4d7f15b976e798473c,
title = {Circular background decreases misunderstanding of multidimensional scaling results for naive readers},
year = {2018},
url = {https://www.semanticscholar.org/paper/7ccfaae1079ee90208ecfb4d7f15b976e798473c},
abstract = {Non-linear multidimensional scaling (NL-MDS) methods are widely used to give an insight on structures of a dataset. Such a technic displays a “map” of data points onto a 2 dimensional space. The reader is expected to have natural understanding of proximity relationships between items. In our experience, MDS are especially helpful as a support for the collaboration between data analysts and specialists of other fields. Indeed, it often allows understanding main issues, major features, how to deal with data and so on. However, we observed that the classical/rectangular display of map causes confusion for non-specialists and long explanation is often required before reaching the fruitful step of the collaboration. The meaning –the absence of meaning, actuallyof axes can be subject for many questions and skepticism from many naive persons. Although it is hardly quantifiable, we observed that using a circle-shaped background for maps improves the understanding of the concept of data mapping by far. We however present here a subjective feedback that may support the practical contribution of NL-MDS for other scientific fields. 1 NL-MDS aims at providing information about data organisation Multi-dimensional scaling (MDS) methods can be defined as the set of methods that display data lying in a metric space (taken into account through a distance matrix) onto an output space while preserving “as much as possible” the distances between them. Of course, there are many ways to set the meaning of “as much as possible”. For example, the classical MDS [1] minimizes the raw square difference between original and output distances when non-linear methods (NL-MDS) assign a higher weight to short distances in order to favor there preservation, which, consequently relax constraints on larger distances. There are a huge number of existing methods proposed in that goal, including Sammon’s mapping [2], Curvilinear Component Analysis [3], Isomap [4], Locally Linear Embedding [5], Local MultiDimensional Scaling [6], Data-Driven High Dimensional Scaling [7] and Stochastic Neighbor Embedding [8, 9] and NeRV [10]. Most often, the output space is a two (or three) dimensional Euclidean space in order to allow an intuitive exploration. Indeed, most of the time, such algorithms aim at providing a better understanding of the dataset. In the present paper, we focus on cases where the output * Corresponding author: sylvain.lespinats@cea.fr © The Authors, published by EDP Sciences. This is an open access article distributed under the terms of the Creative Commons Attribution License 4.0 (http://creativecommons.org/licenses/by/4.0/). MATEC Web of Conferences 189, 10002 (2018) https://doi.org/10.1051/matecconf/201818910002},
author = {F. and S.},
}

@article{20b0c4bcda37407e83bd433f8a1601d428ce49f7,
title = {No Pressure! Addressing the Problem of Local Minima in Manifold Learning Algorithms},
year = {2019},
url = {https://www.semanticscholar.org/paper/20b0c4bcda37407e83bd433f8a1601d428ce49f7},
abstract = {Nonlinear embedding manifold learning methods provide invaluable visual insights into the structure of high-dimensional data. However, due to a complicated nonconvex objective function, these methods can easily get stuck in local minima and their embedding quality can be poor. We propose a natural extension to several manifold learning methods aimed at identifying pressured points, i.e. points stuck in poor local minima and have poor embedding quality. We show that the objective function can be decreased by temporarily allowing these points to make use of an extra dimension in the embedding space. Our method is able to improve the objective function value of existing methods even after they get stuck in a poor local minimum.},
author = {Max Vladymyrov},
arxivid = {1906.11389},
}

@article{57652ac4498e361f53d7bcf1c42529e425a9b445,
title = {k-NN Sampling for Visualization of Dynamic Data Using LION-tSNE},
year = {2019},
url = {https://www.semanticscholar.org/paper/57652ac4498e361f53d7bcf1c42529e425a9b445},
abstract = {Dimensionality reduction algorithms are often used to visualize multi-dimensional data, which are mostly non-parametric. Non-parametric methods do not provide any explicit intuition for adding new data points into an existing environment which limits the applicability of visualization for Big Data scenario. The LION-tSNE (Local Interpolation with Outlier coNtrol t-Distributed Stochastic Neighbor Embedding) method was proposed to overcome the limitations of existing techniques. The LION-tSNE algorithm uses random sampling method for tSNE model design which creates an initial visual environment then new data points are added to this environment using local-IDW(Inverse Distance Weighting) interpolation method. The randomly selected sample data often suffer from non-representativeness of the whole data which creates inconsistency in the tSNE environment. To overcome this problem two new sampling methods are proposed which are based on k-NN (k-Nearest Neighbor) graph update properties. It is empirically shown that proposed methods outperform existing LION-tSNE method with 0.5 to 2% more k-NN accuracy and results are more consistent. The study is done on five differently characterized datasets with three different initial solutions of tSNE. The proposed method results are statistically significant which is done by statistical method pairwise t-test.},
author = {Bheekya Dharamsotu and K. Rani and S. A. Moiz and C. Raghavendra Rao},
doi = {10.1109/HiPC.2019.00019},
}

@article{a95bf1efb0b8ecf29fde9bf417d58a7456fd8d00,
title = {On the Role and Impact of the Metaparameters in t-distributed Stochastic Neighbor Embedding},
year = {2010},
url = {https://www.semanticscholar.org/paper/a95bf1efb0b8ecf29fde9bf417d58a7456fd8d00},
abstract = {Similarity-based embedding is a paradigm that recently gained interest in the field of nonlinear dimensionality reduction. It provides an elegant framework that naturally emphasizes the preservation of the local structure of the data set. An emblematic method in this trend is t-distributed stochastic neighbor embedding (t-SNE), which is acknowledged to be an efficient method in the recent literature. This paper aims at analyzing the reasons of this success, together with the impact of the two metaparameters embedded in the method. Moreover, the paper shows that t-SNE can be interpreted as a distance-preserving method with a specific distance transformation, making the link with existing methods. Experiments on artificial data support the theoretical discussion.},
author = {J. Lee and M. Verleysen},
doi = {10.1007/978-3-7908-2604-3_31},
}

@article{f1fffa10219d91aa107184981a669b18e733b6cf,
title = {Type 1 and 2 symmetric divergences for stochastic neighbor embedding},
year = {2012},
url = {https://www.semanticscholar.org/paper/f1fffa10219d91aa107184981a669b18e733b6cf},
abstract = {Stochastic neighbor embedding (SNE) is a method of dimensionality reduction (DR) that involves softmax similarities measured between all pairs of data points. In order to build a low-dimensional embedding, SNE tries to reproduce the similarities observed in the highdimensional data space. The capability of softmax similarities to fight the phenomenon of norm concentration has been studied in previous work. This paper investigates a complementary aspect, namely, the cost function that quantifies the mismatch between the highand low-dimensional similarities. We show experimentally that switching from a simple KullbackLeibler divergences to mixtures of dual divergences increases the quality of DR. This modification brings SNE to the performance level of its Student t-distributed variant, without the need to resort to non-identical similarity definitions in the highand low-dimensional spaces. These results allow us to conclude that future improvements in similarity-based DR will likely emerge from better definitions of the cost function.},
author = {J. Lee},
}

@article{b4cc2c88aa65e0b82eeade455edead2687f8c281,
title = {Interactive Visual Analysis of Mass Cytometry Data by Hierarchical Stochastic Neighbor Embedding Reveals Rare Cell Types},
year = {2017},
url = {https://www.semanticscholar.org/paper/b4cc2c88aa65e0b82eeade455edead2687f8c281},
abstract = {Mass cytometry allows high-resolution dissection of the cellular composition of the immune system. However, the high-dimensionality, large size, and non-linear structure of the data poses considerable challenges for data analysis. In particular, dimensionality reduction-based techniques like t-SNE offer single-cell resolution but are limited in the number of cells that can be analysed. Here we introduce Hierarchical Stochastic Neighbor Embedding (HSNE) for the analysis of mass cytometry datasets. HSNE constructs a hierarchy of non-linear similarities that can be interactively explored with a stepwise increase in detail up to the single-cell level. We applied HSNE to a study on gastrointestinal disorders and three other available mass cytometry datasets. We found that HSNE efficiently replicates previous observations and identifies rare cell populations that were previously missed due to downsampling. Thus, HSNE removes the scalability limit of conventional t-SNE analysis, a feature that makes it highly suitable for the analysis of massive high-dimensional datasets.},
author = {V. U. van and T. Höllt and Nicola Pezzotti and Na Li and Mjt Reinders and E. Eisemann and A. Vilanova and F. Koning and B. Lelieveldt},
doi = {10.1101/169888},
}

@article{84010c93b04633572ed55c3851662c360d24d6e4,
title = {Graph-Based Dimensionality Reduction},
year = {2012},
url = {https://www.semanticscholar.org/paper/84010c93b04633572ed55c3851662c360d24d6e4},
abstract = {12.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352 12.2 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352 12.3 Classical methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353 12.3.1 Principal component analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354 12.3.2 Multidimensional scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355 12.3.3 Nonlinear MDS and Distance Preservation . . . . . . . . . . . . . . . . . . . . . . . . . . 356 12.4 Nonlinearity through Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357 12.5 Graph-Based Distances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358 12.6 Graph-Based Similarities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361 12.6.1 Laplacian Eigenmaps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361 12.6.2 Locally linear embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364 12.7 Graph embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365 12.7.1 From LD to HD: Self-Organizing Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366 12.7.2 From HD to LD: Isotop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367 12.8 Examples and comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368 12.8.1 Quality Assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369 12.8.2 Data Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370 12.8.3 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371 12.8.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372 12.9 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374},
author = {J. Lee and M. Verleysen},
doi = {10.1201/B12281-12},
}

@article{dd5102a849dd1b5c58675b00dc0bd65d21c79109,
title = {Two key properties of dimensionality reduction methods},
year = {2014},
url = {https://www.semanticscholar.org/paper/dd5102a849dd1b5c58675b00dc0bd65d21c79109},
abstract = {Dimensionality reduction aims at providing faithful low-dimensional representations of high-dimensional data. Its general principle is to attempt to reproduce in a low-dimensional space the salient characteristics of data, such as proximities. A large variety of methods exist in the literature, ranging from principal component analysis to deep neural networks with a bottleneck layer. In this cornucopia, it is rather difficult to find out why a few methods clearly outperform others. This paper identifies two important properties that enable some recent methods like stochastic neighborhood embedding and its variants to produce improved visualizations of high-dimensional data. The first property is a low sensitivity to the phenomenon of distance concentration. The second one is plasticity, that is, the capability to forget about some data characteristics to better reproduce the other ones. In a manifold learning perspective, breaking some proximities typically allow for a better unfolding of data. Theoretical developments as well as experiments support our claim that both properties have a strong impact. In particular, we show that equipping classical methods with the missing properties significantly improves their results.},
author = {J. Lee and M. Verleysen},
doi = {10.1109/CIDM.2014.7008663},
}

@article{b374077ad4913fa7da66538fd7f87392c596d7fc,
title = {Scale-independent quality criteria for dimensionality reduction},
year = {2010},
url = {https://www.semanticscholar.org/paper/b374077ad4913fa7da66538fd7f87392c596d7fc},
abstract = {Dimensionality reduction aims at representing high-dimensional data in low-dimensional spaces, in order to facilitate their visual interpretation. Many techniques exist, ranging from simple linear projections to more complex nonlinear transformations. The large variety of methods emphasizes the need of quality criteria that allow for fair comparisons between them. This paper extends previous work about rank-based quality criteria and proposes to circumvent their scale dependency. Most dimensionality reduction techniques indeed rely on a scale parameter that distinguish between local and global data properties. Such a scale dependency can be similarly found in usual quality criteria: they assess the embedding quality on a certain scale. Experiments with various dimensionality reduction techniques eventually show the strengths and weaknesses of the proposed scale-independent criteria.},
author = {J. Lee and M. Verleysen},
doi = {10.1016/j.patrec.2010.04.013},
}

@article{c733a9ebbeacf7aa42923c8d2ad89ef90d730633,
title = {Symmetric Generative Methods and tSNE: A Short Survey},
year = {2018},
url = {https://www.semanticscholar.org/paper/c733a9ebbeacf7aa42923c8d2ad89ef90d730633},
abstract = {In data visualization, a family of methods is dedicated to the symmetric numerical matrices which contain the distances or similarities between high-dimensional data vectors. The method t-Distributed Stochastic Neighbor Embedding and its variants lead to competitive nonlinear embeddings which are able to reveal the natural classes. For comparisons, it is surveyed the recent probabilistic and model-based alternative methods from the literature (LargeVis, Glove, Latent Space Position Model, probabilistic Correspondence Analysis, Stochastic Block Model) for nonlinear embedding via low dimensional positions.},
author = {R. Priam},
doi = {10.5220/0006684303560363},
}

@article{fbcde17d4ccfdb74da1c17b6df45a41524a7e84f,
title = {Learning image manifold using neighboring similarity integration},
year = {2014},
url = {https://www.semanticscholar.org/paper/fbcde17d4ccfdb74da1c17b6df45a41524a7e84f},
abstract = {The perspective of image manifold and associated manifold learning methods have demonstrated promising results in finding the underlying structure from images in the high dimensional space. Conventional manifold learning methods construct the similarity relationship of image set only based on the pairwise Euclidean distance of images, so they may obtain deceptive similarity and suffer performance degradation. In this paper, we present an Neighboring Similarity Integration(NSI) algorithm to explore image manifold under probability preserving principle. NSI is based on the neighboring similarity of image samples and the local structures of image manifold, and can increases the estimation accuracy of similarity and enhance the learning ability for image manifold. The experimental results of image visualization problem on Yale and MNIST databases are presented to demonstrate the effectiveness of the proposed method.},
author = {Songsong Wu and Xiaoyuan Jing and Jian Yang and Jing-yu Yang},
doi = {10.1109/ICIP.2014.7025380},
}

@article{46b457dfec012fe240e63b99738bd2c8bd690432,
title = {Statistic Software for Neighbor Embedding},
year = {2017},
url = {https://www.semanticscholar.org/paper/46b457dfec012fe240e63b99738bd2c8bd690432},
abstract = {},
author = {Jie Zhao},
}

@article{d64daa7e69d534e66ec0d39941083bf5566582f3,
title = {Some steps towards a general principle for dimensionality reduction mappings},
year = {2010},
url = {https://www.semanticscholar.org/paper/d64daa7e69d534e66ec0d39941083bf5566582f3},
abstract = {In the past years, many dimensionality reduction methods have been established which allow to visualize high dimensional data sets. Recently, also formal evaluation schemes have been proposed for data visualization, which allow a quantitative evaluation along general principles. Most techniques provide a mapping of a priorly given finite set of points only, requiring additional steps for out-of-sample extensions. We propose a general view on dimensionality reduction based on the concept of cost functions, and, based on this general principle, extend dimensionality reduction to explicit mappings of the data manifold. This offers the possibility of simple out-of-sample extensions. Further, it opens a way towards a theory of data visualization taking the perspective of its generalization ability to new data points. We demonstrate the approach based in a simple example.},
author = {B. Hammer and K. Bunte and Michael Biehl},
}

@article{f37c465eee25b0394a770ab82cee622ce7eed440,
title = {Linear basis-function t-SNE for fast nonlinear dimensionality reduction},
year = {2012},
url = {https://www.semanticscholar.org/paper/f37c465eee25b0394a770ab82cee622ce7eed440},
abstract = {t-distributed stochastic neighbor embedding (t-SNE) constitutes a nonlinear dimensionality reduction technique which is particularly suited to visualize high dimensional data sets with intrinsic nonlinear structures. A major drawback, however, consists in its squared complexity which makes the technique infeasible for large data sets or online application in an interactive framework. In addition, since the technique is non parametric, it possesses no direct method to extend the technique to novel data points. In this contribution, we propose an extension of t-SNE to an explicit mapping. In the limit, it reduces to standard non-parametric t-SNE, while offering a feasible nonlinear embedding function for other parameter choices. We evaluate the performance of the technique when trained on a small subpart of the given data only. It turns out that its generalization ability is good when evaluated with the standard quality curve. Further, in many cases, it obtains a quality which approximates the quality of t-SNE when trained on the full data set, albeit only 10% of the data are used for training. This opens the way towards efficient nonlinear dimensionality reduction techniques as required in interactive settings.},
author = {A. Gisbrecht and B. Mokbel and B. Hammer},
doi = {10.1109/IJCNN.2012.6252809},
}

@article{237b1f74b02f0cc59bffbabbc8706871224de926,
title = {A General Framework for Dimensionality Reduction for Large Data Sets},
year = {2011},
url = {https://www.semanticscholar.org/paper/237b1f74b02f0cc59bffbabbc8706871224de926},
abstract = {With electronic data increasing dramatically in almost all areas of research, a plethora of new techniques for automatic dimensionality reduction and data visualization has become available in recent years. These offer an interface which allows humans to rapidly scan through large volumes of data. With data sets becoming larger and larger, however, the standard methods can no longer be applied directly. Random subsampling or prior clustering still being one of the most popular solutions in this case, we discuss a principled alternative and formalize the approaches under a general perspectives of dimensionality reduction as cost optimization. We have a first look at the question whether these techniques can be accompanied by theoretical guarantees.},
author = {B. Hammer and Michael Biehl and K. Bunte and B. Mokbel},
doi = {10.1007/978-3-642-21566-7_28},
}

@article{4081a0fa7b9ddad4058c962fc7068fea1f8669bd,
title = {A General Framework for Dimensionality-Reducing Data Visualization Mapping},
year = {2012},
url = {https://www.semanticscholar.org/paper/4081a0fa7b9ddad4058c962fc7068fea1f8669bd},
abstract = {In recent years, a wealth of dimension-reduction techniques for data visualization and preprocessing has been established. Nonparametric methods require additional effort for out-of-sample extensions, because they provide only a mapping of a given finite set of points. In this letter, we propose a general view on nonparametric dimension reduction based on the concept of cost functions and properties of the data. Based on this general principle, we transfer nonparametric dimension reduction to explicit mappings of the data manifold such that direct out-of-sample extensions become possible. Furthermore, this concept offers the possibility of investigating the generalization ability of data visualization to new data points. We demonstrate the approach based on a simple global linear mapping, as well as prototype-based local linear mappings. In addition, we can bias the functional form according to given auxiliary information. This leads to explicit supervised visualization mappings with discriminative properties comparable to state-of-the-art approaches.},
author = {K. Bunte and Michael Biehl and B. Hammer},
doi = {10.1162/NECO_a_00250},
}

@article{7da7c81d23d2afa554df5c7ec5e4b82c85f46004,
title = {Data visualization by nonlinear dimensionality reduction},
year = {2015},
url = {https://www.semanticscholar.org/paper/7da7c81d23d2afa554df5c7ec5e4b82c85f46004},
abstract = {In this overview, commonly used dimensionality reduction techniques for data visualization and their properties are reviewed. Thereby, the focus lies on an intuitive understanding of the underlying mathematical principles rather than detailed algorithmic pipelines. Important mathematical properties of the technologies are summarized in the tabular form. The behavior of representative techniques is demonstrated for three benchmarks, followed by a short discussion on how to quantitatively evaluate these mappings. In addition, three currently active research topics are addressed: how to devise dimensionality reduction techniques for complex non‐vectorial data sets, how to easily shape dimensionality reduction techniques according to the users preferences, and how to device models that are suited for big data sets. WIREs Data Mining Knowl Discov 2015, 5:51–73. doi: 10.1002/widm.1147},
author = {A. Gisbrecht and B. Hammer},
doi = {10.1002/widm.1147},
}

@article{4383921864307f3039cb6165983f81cf86edc3bf,
title = {Dimensionality reduction mappings},
year = {2011},
url = {https://www.semanticscholar.org/paper/4383921864307f3039cb6165983f81cf86edc3bf},
abstract = {A wealth of powerful dimensionality reduction methods has been established which can be used for data visualization and preprocessing. These are accompanied by formal evaluation schemes, which allow a quantitative evaluation along general principles and which even lead to further visualization schemes based on these objectives. Most methods, however, provide a mapping of a priorly given finite set of points only, requiring additional steps for out-of-sample extensions. We propose a general view on dimensionality reduction based on the concept of cost functions, and, based on this general principle, extend dimensionality reduction to explicit mappings of the data manifold. This offers simple out-of-sample extensions. Further, it opens a way towards a theory of data visualization taking the perspective of its generalization ability to new data points. We demonstrate the approach based on a simple global linear mapping as well as prototype-based local linear mappings.},
author = {K. Bunte and Michael Biehl and B. Hammer},
doi = {10.1109/CIDM.2011.5949443},
}

@article{e3f89fd339facf4000cc26b67ee7c09dfa1a4a4c,
title = {Visualizing the quality of dimensionality reduction},
year = {2013},
url = {https://www.semanticscholar.org/paper/e3f89fd339facf4000cc26b67ee7c09dfa1a4a4c},
abstract = {The growing number of dimensionality reduction methods available for data visualization has recently inspired the development of formal measures to evaluate the resulting low-dimensional representation independently from the methods' inherent criteria. Many evaluation measures can be summarized based on the co-ranking matrix. In this work, we analyze the characteristics of the co-ranking framework, focusing on interpretability and controllability in evaluation scenarios where a fine-grained assessment of a given visualization is desired. We extend the framework in two ways: (i) we propose how to link the evaluation to point-wise quality measures which can be used directly to augment the evaluated visualization and highlight erroneous regions; (ii) we improve the parameterization of the quality measure to offer more direct control over the evaluation's focus, and thus help the user to investigate more specific characteristics of the visualization.},
author = {B. Mokbel and W. Lueks and A. Gisbrecht and B. Hammer},
doi = {10.1016/j.neucom.2012.11.046},
}

@article{f318326fb5c1a8a17a6f833df36171bd8fc33b0d,
title = {PixelSNE: Pixel‐Aligned Stochastic Neighbor Embedding for Efficient 2D Visualization with Screen‐Resolution Precision},
year = {2018},
url = {https://www.semanticscholar.org/paper/f318326fb5c1a8a17a6f833df36171bd8fc33b0d},
abstract = {Embedding and visualizing large‐scale high‐dimensional data in a two‐dimensional space is an important problem, because such visualization can reveal deep insights of complex data. However, most of the existing embedding approaches run on an excessively high precision, even when users want to obtain a brief insight from a visualization of large‐scale datasets, ignoring the fact that in the end, the outputs are embedded onto a fixed‐range pixel‐based screen space. Motivated by this observation and directly considering the properties of screen space in an embedding algorithm, we propose Pixel‐Aligned Stochastic Neighbor Embedding (PixelSNE), a highly efficient screen resolution‐driven 2D embedding method which accelerates Barnes‐Hut tree‐based t‐distributed stochastic neighbor embedding (BH‐SNE), which is known to be a state‐of‐the‐art 2D embedding method. Our experimental results show a significantly faster running time for PixelSNE compared to BH‐SNE for various datasets while maintaining comparable embedding quality.},
author = {Minjeong Kim and Min-Je Choi and Sunwoong Lee and Jian Tang and Haesun Park and J. Choo},
doi = {10.1111/cgf.13418},
}

@article{bcd96014a216e29ff55ecb2144c72aec273ea1a4,
title = {Facilitate the Parametric Dimension Reduction by Gradient Clipping},
year = {2020},
url = {https://www.semanticscholar.org/paper/bcd96014a216e29ff55ecb2144c72aec273ea1a4},
abstract = {We extend a well-known dimension reduction method, t-distributed stochastic neighbor embedding (t-SNE), from non-parametric to parametric by training neural networks. The main advantage of a parametric technique is the generalization of handling new data, which is particularly beneficial for streaming data exploration. However, training a neural network to optimize the t-SNE objective function frequently fails. Previous methods overcome this problem by pre-training and then fine-tuning the network. We found that the training failure comes from the gradient exploding problem, which occurs when data points distant in high-dimensional space are projected to nearby embedding positions. Accordingly, we applied the gradient clipping method to solve the problem. Since the networks are trained by directly optimizing the t-SNE objective function, our method achieves an embedding quality that is compatible with the non-parametric t-SNE while enjoying the ability of generalization. Due to mini-batch network training, our parametric dimension reduction method is highly efficient. We further extended other non-parametric state-of-the-art approaches, such as LargeVis and UMAP, to the parametric versions. Experiment results demonstrate the feasibility of our method. Considering its practicability, we will soon release the codes for public use.},
author = {Chien-Hsun Lai and Yu-Shuen Wang},
arxivid = {2009.14373},
}

@article{f00b186af75fa181f20d30c30ab84d5e36dcfeb8,
title = {Supervised dimension reduction mappings},
year = {2011},
url = {https://www.semanticscholar.org/paper/f00b186af75fa181f20d30c30ab84d5e36dcfeb8},
abstract = {Abstract. We propose a general principle to extend dimension reduction tools to explicit dimension reduction mappings and we show that this can serve as an interface to incorporate prior knowledge in the form of class labels. We explicitly demonstrate this technique by combining locally linear mappings which result from matrix learning vector quantization schemes with the t-distributed stochastic neighbor embedding cost function. The technique is tested on several benchmark data sets.},
author = {K. Bunte and Michael Biehl and B. Hammer},
}

@article{e99549886039616431a1d0f856016f37852aa514,
title = {Type 1 and 2 mixtures of Kullback-Leibler divergences as cost functions in dimensionality reduction based on similarity preservation},
year = {2013},
url = {https://www.semanticscholar.org/paper/e99549886039616431a1d0f856016f37852aa514},
abstract = {Stochastic neighbor embedding (SNE) and its variants are methods of dimensionality reduction (DR) that involve normalized softmax similarities derived from pairwise distances. These methods try to reproduce in the low-dimensional embedding space the similarities observed in the high-dimensional data space. Their outstanding experimental results, compared to previous state-of-the-art methods, originate from their capability to foil the curse of dimensionality. Previous work has shown that this immunity stems partly from a property of shift invariance that allows appropriately normalized softmax similarities to mitigate the phenomenon of norm concentration. This paper investigates a complementary aspect, namely, the cost function that quantifies the mismatch between similarities computed in the high- and low-dimensional spaces. Stochastic neighbor embedding and its variant t-SNE rely on a single Kullback-Leibler divergence, whereas a weighted mixture of two dual KL divergences is used in neighborhood retrieval and visualization (NeRV). We propose in this paper a different mixture of KL divergences, which is a scaled version of the generalized Jensen-Shannon divergence. We show experimentally that this divergence produces embeddings that better preserve small K-ary neighborhoods, as compared to both the single KL divergence used in SNE and t-SNE and the mixture used in NeRV. These results allow us to conclude that future improvements in similarity-based DR will likely emerge from better definitions of the cost function.},
author = {J. Lee and Emilie Renard and G. Bernard and P. Dupont and M. Verleysen},
doi = {10.1016/j.neucom.2012.12.036},
}

@article{b016193587df9e9bf3c33f93be00589ed7fbc3db,
title = {Unsupervised dimensionality reduction: the challenge of big data visualization},
year = {2015},
url = {https://www.semanticscholar.org/paper/b016193587df9e9bf3c33f93be00589ed7fbc3db},
abstract = {Dimensionality reduction is an unsupervised task that allows high-dimensional data to be processed or visualised in lower-dimensional spaces. This tutorial reviews the basic principles of dimensionality reduc- tion and discusses some of the approaches that were published over the past years from the perspective of their application to big data. The tu- torial ends with a short review of papers about dimensionality reduction in these proceedings, as well as some perspectives for the near future.},
author = {K. Bunte and J. Lee},
}

@article{023d966d8297a366beff3f5e91eddfd390a7de13,
title = {Visualizing and Exploring Dynamic High-Dimensional Datasets with LION-tSNE},
year = {2017},
url = {https://www.semanticscholar.org/paper/023d966d8297a366beff3f5e91eddfd390a7de13},
abstract = {T-distributed stochastic neighbor embedding (tSNE) is a popular and prize-winning approach for dimensionality reduction and visualizing high-dimensional data. However, tSNE is non-parametric: once visualization is built, tSNE is not designed to incorporate additional data into existing representation. It highly limits the applicability of tSNE to the scenarios where data are added or updated over time (like dashboards or series of data snapshots). 
In this paper we propose, analyze and evaluate LION-tSNE (Local Interpolation with Outlier coNtrol) - a novel approach for incorporating new data into tSNE representation. LION-tSNE is based on local interpolation in the vicinity of training data, outlier detection and a special outlier mapping algorithm. We show that LION-tSNE method is robust both to outliers and to new samples from existing clusters. We also discuss multiple possible improvements for special cases. 
We compare LION-tSNE to a comprehensive list of possible benchmark approaches that include multiple interpolation techniques, gradient descent for new data, and neural network approximation.},
author = {Andrey Boytsov and François Fouquet and Thomas Hartmann and Yves Le Traon},
arxivid = {1708.04983},
}

@article{853ff12baceb069f3849b26b70743eb51e640bc6,
title = {Simbed: Similarity-Based Embedding},
year = {2009},
url = {https://www.semanticscholar.org/paper/853ff12baceb069f3849b26b70743eb51e640bc6},
abstract = {Simbed, standing for similarity-based embedding, is a new method of embedding high-dimensional data. It relies on the preservation of pairwise similarities rather than distances. In this respect, Simbed can be related to other techniques such as stochastic neighbor embedding and its variants. A connection with curvilinear component analysis is also pointed out. Simbed differs from these methods by the way similarities are defined and compared in both the data and embedding spaces. In particular, similarities in Simbed can account for the phenomenon of norm concentration that occurs in high-dimensional spaces. This feature is shown to reinforce the advantage of Simbed over other embedding techniques in experiments with a face database.},
author = {J. Lee and M. Verleysen},
doi = {10.1007/978-3-642-04277-5_10},
}

@article{3a24c276368fa63473078723ce4bc99c9ea36019,
title = {Stability Comparison of Dimensionality Reduction Techniques Attending to Data and Parameter Variations},
year = {2013},
url = {https://www.semanticscholar.org/paper/3a24c276368fa63473078723ce4bc99c9ea36019},
abstract = {The analysis of the big volumes of data requires efficient and robust dimension reduction techniques to represent data into lower-dimensional spaces, which ease human understanding. This paper presents a study of the stability, robustness and performance of some of these dimension reduction algorithms with respect to algorithm and data parameters, which usually have a major influence in the resulting embeddings. This analysis includes the performance of a large panel of techniques on both artificial and real datasets, focusing on the geometrical variations experimented when changing different parameters. The results are presented by identifying the visual weaknesses of each technique, providing some suitable data-processing tasks to enhance the stability.},
author = {Francisco J. García-Fernández and M. Verleysen and J. Lee and Ignacio Díaz Blanco},
doi = {10.2312/PE.VAMP.VAMP2013.005-009},
}

@article{b2c34615f72bdddb6d5fb051b42872afe25b95d0,
title = {Performance study of dimensionality reduction methods for metrology of nonrigid mechanical parts},
year = {2013},
url = {https://www.semanticscholar.org/paper/b2c34615f72bdddb6d5fb051b42872afe25b95d0},
abstract = {The geometric measurement of parts using a coordinate measuring machine (CMM) has been generally adapted to the advanced automotive and aerospace industries. However, for the geometric inspection of deformable free-form parts, special inspection fixtures, in combination with CMM’s and/or optical data acquisition devices (scanners), are used. As a result, the geometric inspection of flexible parts is a consuming process in terms of time and money. The general procedure to eliminate the use of inspection fixtures based on distance preserving nonlinear dimensionality reduction (NLDR) technique was developed in our previous works. We sought out geometric properties that are invariant to inelastic deformations. In this paper we will only present a systematic comparison of some well-known dimensionality reduction techniques in order to evaluate their accuracy and potential for non-rigid metrology. We will demonstrate that even though these techniques may provide acceptable results through artificial data on certain fields like pattern recognition and machine learning, this performance cannot be extended to all real engineering metrology problems where high accuracy is needed.},
author = {Hassan Radvar-Esfahlan and S. Tahan},
doi = {10.1051/IJMQE/2013051},
}

@article{479e4d1a4820b49bfe5fb6d28f81aed57aa8963c,
title = {Chapter 1 Graph-based dimensionality reduction},
year = {2011},
url = {https://www.semanticscholar.org/paper/479e4d1a4820b49bfe5fb6d28f81aed57aa8963c},
abstract = {},
author = {J. Lee},
}
