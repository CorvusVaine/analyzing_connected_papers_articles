@article{442ff65682477151be4723abba16df7a85153e5f,
title = {RNN-VirSeeker: A Deep Learning Method for Identification of Short Viral Sequences From Metagenomes},
year = {2020},
url = {https://www.semanticscholar.org/paper/442ff65682477151be4723abba16df7a85153e5f},
abstract = {Viruses are the most abundant biological entities on earth, and play vital roles in many aspects of microbial communities. As major human pathogens, viruses have caused huge mortality and morbidity to human society in history. Metagenomic sequencing methods could capture all microorganisms from microbiota, with sequences of viruses mixed with these of other species. Therefore, it is necessary to identify viral sequences from metagenomes. However, existing methods perform poorly on identifying short viral sequences. To solve this problem, a deep learning based method, RNN-VirSeeker, is proposed in this paper. RNN-VirSeeker was trained by sequences of 500bp sampled from known Virus and Host RefSeq genomes. Experimental results on the testing set have shown that RNN-VirSeeker exhibited AUROC of 0.9175, recall of 0.8640 and precision of 0.9211 for sequences of 500bp, and outperformed three widely used methods, VirSorter, VirFinder, and DeepVirFinder, on identifying short viral sequences. RNN-VirSeeker was also used to identify viral sequences from a CAMI dataset and a human gut metagenome. Compared with DeepVirFinder, RNN-VirSeeker identified more viral sequences from these metagenomes and achieved greater values of AUPRC and AUROC. RNN-VirSeeker is freely available at https://github.com/crazyinter/RNN-VirSeeker.},
author = {Fu Liu and Yan Miao and Yun Liu and Tao Hou},
doi = {10.1109/TCBB.2020.3044575},
pmid = {33315571},
}

@article{7a8de9c8840ff9370d90032abad8fdbc00332b0b,
title = {VFM: Identification of Bacteriophages From Metagenomic Bins and Contigs Based on Features Related to Gene and Genome Composition},
year = {2019},
url = {https://www.semanticscholar.org/paper/7a8de9c8840ff9370d90032abad8fdbc00332b0b},
abstract = {As the main regulator of microbial community composition, bacteriophages exist widely on Earth. However, since they are hidden in metagenomes, most of them are unknown. To identify phages from metagenomes more effectively, a new tool named VFM (Virus Finding & Mining) is presented in this paper. VFM has two versions, i.e., bin-VFM and unbin-VFM. Eighteen new features describing the codon usage bias, the proportion of hits of clusters of orthologous groups of proteins (COG), and 1-mer and 2-mer frequency are introduced to improve the performance of the classifiers. By using missing value interpolation, bin-VFM improves the classification performance for short sequence bins significantly. Compared with previous tools for virus mining, bin-VFM and unbin-VFM perform much better for simulated and real metagenomes with short and long sequences respectively. Thus, VFM may play a helpful role in studies of metagenome-related problems, such as horizontal gene transfer and antibiotic resistance. VFM is freely available at https://github.com/liuql2019/VFM.},
author = {Qiaoliang Liu and Fu Liu and Jiaxue He and Miaolei Zhou and Tao Hou and Yun Liu},
doi = {10.1109/ACCESS.2019.2957833},
}

@article{fe85a00ab13545a927990086e0a0107e3a49efb4,
title = {Graph Kernel Attention Transformers},
year = {2021},
url = {https://www.semanticscholar.org/paper/fe85a00ab13545a927990086e0a0107e3a49efb4},
abstract = {We introduce a new class of graph neural networks (GNNs), by combining several concepts that were so far studied independently graph kernels, attention-based networks with structural priors and more recently, efficient Transformers architectures applying small memory footprint implicit attention methods via low rank decomposition techniques. The goal of the paper is twofold. Proposed by us Graph Kernel Attention Transformers (or GKATs) are much more expressive than SOTA GNNs as capable of modeling longer-range dependencies within a single layer. Consequently, they can use more shallow architecture design. Furthermore, GKAT attention layers scale linearly rather than quadratically in the number of nodes of the input graphs, even when those graphs are dense, requiring less compute than their regular graph attention counterparts. They achieve it by applying new classes of graph kernels admitting random feature map decomposition via random walks on graphs. As a byproduct of the introduced techniques, we obtain a new class of learnable graph sketches, called graphots, compactly encoding topological graph properties as well as nodes’ features. We conducted exhaustive empirical comparison of our method with nine different GNN classes on tasks ranging from motif detection through social network classification to bioinformatics challenges, showing consistent gains coming from GKATs.},
author = {K. Choromanski and Han Lin and Haoxian Chen and Jack Parker-Holder},
}

@article{25c0e37f3bcaaea4527c9d78c297461bd0eb6693,
title = {Genome Assembly Using Reinforcement Learning},
year = {2019},
url = {https://www.semanticscholar.org/paper/25c0e37f3bcaaea4527c9d78c297461bd0eb6693},
abstract = {Reinforcement learning (RL) aims to build intelligent agents able to optimally act after the training process to solve a given goal task in an autonomous and non-deterministic fashion. It has been successfully employed in several areas; however, few RL-based approaches related to genome assembly have been found, especially when considering real input datasets. De novo genome assembly is a crucial step in a number of genome projects, but due to its high complexity, the outcome of state-of-art assemblers is still insufficient to assist researchers in answering all their scientific questions properly. Hence, the development of better assembler is desirable and perhaps necessary, and preliminary studies suggest that RL has the potential to solve this computational task. In this sense, this paper presents an empirical analysis to evaluate this hypothesis, particularly in higher scale, through performance assessment along with time and space complexity analysis of a theoretical approach to the problem of assembly proposed by [2] using the RL algorithm Q-learning. Our analysis shows that, although space and time complexities are limiting scale issues, RL is shown as a viable alternative for solving the DNA fragment assembly problem.},
author = {Roberto Xavier and K. Souza and A. Chateau and Ronnie Alves},
doi = {10.1007/978-3-030-46417-2_2},
}

@article{75c51d553475e3e27e1eee893f3e0a41c8ad5920,
title = {What’s in a Genome at NCBI?},
year = {2013},
url = {https://www.semanticscholar.org/paper/75c51d553475e3e27e1eee893f3e0a41c8ad5920},
abstract = {It seems like a simple request to a bioinformatics center like NCBI—“Download the human genome”, “Display the HIV-1 genome”—and yet this is a complicated question in terms of biology, experimental data, the current state of knowledge, and the use to which a particular scientist may wish to put the data. This chapter provides an introduction to those questions, a brief history of genome representations at NCBI as the state of the science evolved over the last few decades, and a summary of some of the many resources and tools that are relevant to Genomes at NCBI.},
author = {J. Ostell},
}

@article{7fecbdefaf522f8ebfe1abccba67492c97c0b912,
title = {An Empirical Study of Neural Network Hyperparameters},
year = {2020},
url = {https://www.semanticscholar.org/paper/7fecbdefaf522f8ebfe1abccba67492c97c0b912},
abstract = {The learning algorithms related to deep learning involves many attributes called hyperparameters, these variables help in determining the network structure. The performance of algorithms depends upon these hyper-parameter variables that are needed to be set prior to the actual implementation of the algorithm. This study involves an overview of some of the commonly used hyperparameters in the context of learning algorithms used for training neural networks along with the analysis of adaptive learning algorithms used for tuning learning rates.},
author = {Aditya Makwe and A. Rathore},
doi = {10.1007/978-981-15-5788-0_36},
}

@article{7d5d2492edbad301787d926d5dd0c95a56da8dd9,
title = {Janggu - Deep learning for genomics},
year = {2019},
url = {https://www.semanticscholar.org/paper/7d5d2492edbad301787d926d5dd0c95a56da8dd9},
abstract = {Motivation In recent years, numerous applications have demonstrated the potential of deep learning for an improved understanding of biological processes. However, most deep learning tools developed so far are designed to address a specific question on a fixed dataset and/or by a fixed model architecture. Adapting these models to integrate new datasets or to address different hypotheses can lead to considerable software engineering effort. To address this aspect we have built Janggu, a python library that facilitates deep learning for genomics applications. Results Janggu aims to ease data acquisition and model evaluation in multiple ways. Among its key features are special dataset objects, which form a unified and flexible data acquisition and pre-processing framework for genomics data that enables streamlining of future research applications through reusable components. Through a numpy-like interface, the dataset objects are directly compatible with popular deep learning libraries, including keras. Furthermore, Janggu offers the possibility to visualize predictions as genomic tracks or by exporting them to the BIGWIG format. We illustrate the functionality of Janggu on several deep learning genomics applications. First, we evaluate different model topologies for the task of predicting binding sites for the transcription factor JunD. Second, we demonstrate the framework on published models for predicting chromatin effects. Third, we show that promoter usage measured by CAGE can be predicted using DNase hyper-sensitivity, histone modifications and DNA sequence features. We improve the performance of these models due to a novel feature in Janggu that allows us to include high-order sequence features. We believe that Janggu will help to significantly reduce repetitive programming overhead for deep learning applications in genomics, while at the same time enabling computational biologists to assess biological hypotheses more rapidly. Availability Janggu is freely available under a GPL-v3 license on https://github.com/BIMSBbioinfo/janggu or via https://pypi.org/project/janggu},
author = {W. Kopp and Remo Monti and Annalaura Tamburrini and U. Ohler and A. Akalin},
doi = {10.1101/700450},
}

@article{abe1058f05b0225d47be9aab8189e843de75373f,
title = {A Pan-Specific GRU-Based Recurrent Neural Network for Predicting HLA-I-Binding Peptides},
year = {2020},
url = {https://www.semanticscholar.org/paper/abe1058f05b0225d47be9aab8189e843de75373f},
abstract = {Human leukocyte antigens (HLAs) play a critical role in human-acquired immune responses by the recognition of non-self-peptides derived from exogenous bacteria, fungi, virus, and so forth. The accurate prediction of HLA-binding peptides is thus extremely useful for the mechanistic research of cell-mediated immunity and related epitope-based vaccine design. In this work, a simple pan-specific gated recurrent unit (GRU)-based recurrent neural network model was successfully proposed for predicting HLA-I-binding peptides. In comparison with the available six allele-specific, four pan-specific, and two ensemble-based prediction models, the GRU model achieves the highest area under the receiver operating characteristic curve (AUC) scores for 21 of 64 entries of the test benchmark datasets. Besides, the GRU model also achieves satisfactory performance on other 24 entries, of which the AUC scores differ by less than 0.1 from the highest scores. Overall, taking the advantages of the GRU network and auto-embedding techniques into account, the established pan-specific GRU model is more simple and direct and shows satisfactory prediction performance for HLA-I-binding peptides with varying lengths.},
author = {Yu Heng and Zuyin Kuang and Shuheng Huang and Linxin Chen and Tingting Shi and Lei Xu and H. Mei},
doi = {10.1021/acsomega.0c02039},
pmid = {32743207},
}

@article{00affd3c59110503c7df938e380ea92d46c0eabe,
title = {Graph Pooling with Representativeness},
year = {2020},
url = {https://www.semanticscholar.org/paper/00affd3c59110503c7df938e380ea92d46c0eabe},
abstract = {Graph Neural Networks (GNNs), which extend deep neural networks to graph-structured data, have attracted increasing attention. They have been proven to be powerful for numerous graph related tasks such as graph classification, link prediction, and node classification. To adapt GNNs to graph classification, recent works aim to learn graph-level representation through a hierarchical pooling procedure. One major direction is to select important nodes to hierarchically coarsen the input graph and gradually reduce the information into the graph representation. However, most of the existing methods only select important nodes, which can be redundant and cannot represent the original graph well. Meanwhile, the information of non-selected nodes is often overlooked when generating a new coarser graph, which may lead to the tremendous loss of important structural and node feature information. In this paper, we propose a novel pooling operator RepPool to learn hierarchical graph representations. Specifically, we introduce the concept of representativeness that is combined with the importance for node selection and we provide a learnable way to integrate non-selected nodes. By combining the RepPool operator with conventional GCN convolutional layers, a hierarchical graph classification architecture is developed. Extensive experiments on various public benchmarks have demonstrated the effectiveness of the proposed method. The implementation of the proposed framework is available11https://github.com/Juanhui28/RepPool/tree/master/RepPool.},
author = {Juanhui Li and Yao Ma and Yiqi Wang and C. Aggarwal and Changdong Wang and Jiliang Tang},
doi = {10.1109/ICDM50108.2020.00039},
}

@article{b8114c94729394af836d27641864ec9502e3efba,
title = {Pooling in Graph Convolutional Neural Networks},
year = {2019},
url = {https://www.semanticscholar.org/paper/b8114c94729394af836d27641864ec9502e3efba},
abstract = {Graph convolutional neural networks (GCNNs) are a powerful extension of deep learning techniques to graph-structured data problems. We empirically evaluate several pooling methods for GCNNs, and combinations of those graph pooling methods with three different architectures: GCN, TAGCN, and GraphSAGE. We confirm that graph pooling, especially DiffPool, improves classification accuracy on popular graph classification datasets and find that, on average, TAGCN achieves comparable or better accuracy than GCN and GraphSAGE, particularly for datasets with larger and sparser graph structures.},
author = {Mark Cheung and John Shi and L. Jiang and Oren Wright and J. Moura},
doi = {10.1109/IEEECONF44664.2019.9048796},
arxivid = {2004.03519},
}

@article{7fbd30fd421d514e8c5496f3cdd98635049bc7e5,
title = {An End-to-End Multiplex Graph Neural Network for Graph Representation Learning},
year = {2021},
url = {https://www.semanticscholar.org/paper/7fbd30fd421d514e8c5496f3cdd98635049bc7e5},
abstract = {Research on graph classification tasks based on graph neural networks has attracted wide attention. The graphs to be classified may have various graph sizes (i.e., different numbers of nodes and edges) and have various graph properties (e.g., average node degree, diameter, and clustering coefficient). The diverse property of graphs has imposed significant challenges on existing graph learning techniques since diverse graphs have different best-fit hyperparameters. Consequently, it is unreasonable to learn graph representation from a set of diverse graphs by a unified graph neural network. Inspired by this, we design an end-to-end Multiplex Graph Neural Network (MxGNN) that learns graph representations with multiple GNNs, and combines them with a learnable method. The main challenge lies with the combination of multiple representation results. Our new findings show that the a priori graph properties do have an effect on the quality of representation learning, which can be used to guide the learning. Our experiments on graph classification with multiple data sets show that the performance of MxGNN is better than the existing graph representation learning methods.},
author = {Yanyan Liang and Yanfeng Zhang and Dechao Gao and Qian Xu},
doi = {10.1109/ACCESS.2021.3070690},
}

@article{8c3bd1381a73e1a9f063b18ffcc236a13c045a94,
title = {Multi-Channel Graph Convolutional Networks},
year = {2019},
url = {https://www.semanticscholar.org/paper/8c3bd1381a73e1a9f063b18ffcc236a13c045a94},
abstract = {Graph neural networks (GNN) has been demonstrated to be effective in classifying graph structures. To further improve the graph representation learning ability, hierarchical GNN has been explored. It leverages the differentiable pooling to cluster nodes into fixed groups, and generates a coarse-grained structure accompanied with the shrinking of the original graph. However, such clustering would discard some graph information and achieve the suboptimal results. It is because the node inherently has different characteristics or roles, and two non-isomorphic graphs may have the same coarse-grained structure that cannot be distinguished after pooling. To compensate the loss caused by coarse-grained clustering and further advance GNN, we propose a multi-channel graph convolutional networks (MuchGCN). It is motivated by the convolutional neural networks, at which a series of channels are encoded to preserve the comprehensive characteristics of the input image. Thus, we define the specific graph convolutions to learn a series of graph channels at each layer, and pool graphs iteratively to encode the hierarchical structures. Experiments have been carefully carried out to demonstrate the superiority of MuchGCN over the state-of-the-art graph classification algorithms.},
author = {Kaixiong Zhou and Qingquan Song and Xiao Huang and D. Zha and Na Zou and Xia Hu},
arxivid = {1912.08306},
}

@article{06d8db84a1b009634bec6fc4c2f6a566f26f8292,
title = {Towards Expressive Graph Representation},
year = {2020},
url = {https://www.semanticscholar.org/paper/06d8db84a1b009634bec6fc4c2f6a566f26f8292},
abstract = {Graph Neural Network (GNN) aggregates the neighborhood of each node into the node embedding and shows its powerful capability for graph representation learning. However, most existing GNN variants aggregate the neighborhood information in a fixed non-injective fashion, which may map different graphs or nodes to the same embedding, reducing the model expressiveness. We present a theoretical framework to design a continuous injective set function for neighborhood aggregation in GNN. Using the framework, we propose expressive GNN that aggregates the neighborhood of each node with a continuous injective set function, so that a GNN layer maps similar nodes with similar neighborhoods to similar embeddings, different nodes to different embeddings and the equivalent nodes or isomorphic graphs to the same embeddings. Moreover, the proposed expressive GNN can naturally learn expressive representations for graphs with continuous node attributes. We validate the proposed expressive GNN (ExpGNN) for graph classification on multiple benchmark datasets including simple graphs and attributed graphs. The experimental results demonstrate that our model achieves state-of-the-art performances on most of the benchmarks.},
author = {Chengsheng Mao and Liang Yao and Yuan Luo},
arxivid = {2010.05427},
}

@article{ab67cbed638a5232d9e875bfaa9218ff35e4458e,
title = {Topology-Aware Graph Signal Sampling for Pooling in Graph Neural Networks},
year = {2021},
url = {https://www.semanticscholar.org/paper/ab67cbed638a5232d9e875bfaa9218ff35e4458e},
abstract = {As a generalization of convolutional neural networks to graph-structured data, graph convolutional networks learn feature embeddings based on the information of each nodes local neighborhood. However, due to the inherent irregularity of such data, extracting hierarchical representations of a graph becomes a challenging task. Several pooling approaches have been introduced to address this issue. In this paper, we propose a novel topology-aware graph signal sampling method to specify the nodes that represent the communities of a graph. Our method selects the sampling set based on the local variation of the signal of each node while considering vertex-domain distances of the nodes in the sampling set. In addition to the interpretability of the sampled nodes provided by our method, the experimental results both on stochastic block models and real-world dataset benchmarks show that our method achieves competitive results compared to the state-of-the-art in the graph classification task.},
author = {Amirhossein Nouranizadeh and M. Matinkia and M. Rahmati},
doi = {10.1109/CSICC52343.2021.9420547},
}

@article{a02f11a576159f21ab50d242fd51ec9c8c86bf4e,
title = {Graph Kernel Neural Networks},
year = {2021},
url = {https://www.semanticscholar.org/paper/a02f11a576159f21ab50d242fd51ec9c8c86bf4e},
abstract = {The convolution operator at the core of many modern neural architectures can effectively be seen as performing a dot product between an input matrix and a filter. While this is readily applicable to data such as images, which can be represented as regular grids in the Euclidean space, extending the convolution operator to work on graphs proves more challenging, due to their irregular structure. In this paper we propose to use graph kernels, i.e., kernel functions that compute an inner product on graphs, to extend the standard convolution operator to the graph domain. This allows us to define an entirely structural model that does not require computing the embedding of the input graph. Our architecture allows to plug-in any type and number of graph kernels and has the added benefit of providing some interpretability in terms of the structural masks that are learned during the training process, similarly to what happens for convolutional masks in traditional convolutional neural networks. We perform an extensive ablation study to investigate the impact of the model hyper-parameters and we show that our model achieves competitive performance on standard graph classification datasets.},
author = {L. Cosmo and G. Minello and M. Bronstein and E. Rodolà and L. Rossi and A. Torsello},
arxivid = {2112.07436},
}

@article{6541afa4b4061a7d5c8387514bedea9dc249fd80,
title = {Invariant and Equivariant Graph Networks},
year = {2018},
url = {https://www.semanticscholar.org/paper/6541afa4b4061a7d5c8387514bedea9dc249fd80},
abstract = {Invariant and equivariant networks have been successfully used for learning images, sets, point clouds, and graphs. A basic challenge in developing such networks is finding the maximal collection of invariant and equivariant linear layers. Although this question is answered for the first three examples (for popular transformations, at-least), a full characterization of invariant and equivariant linear layers for graphs is not known. 
In this paper we provide a characterization of all permutation invariant and equivariant linear layers for (hyper-)graph data, and show that their dimension, in case of edge-value graph data, is 2 and 15, respectively. More generally, for graph data defined on k-tuples of nodes, the dimension is the k-th and 2k-th Bell numbers. Orthogonal bases for the layers are computed, including generalization to multi-graph data. The constant number of basis elements and their characteristics allow successfully applying the networks to different size graphs. From the theoretical point of view, our results generalize and unify recent advancement in equivariant deep learning. In particular, we show that our model is capable of approximating any message passing neural network 
Applying these new linear layers in a simple deep neural network framework is shown to achieve comparable results to state-of-the-art and to have better expressivity than previous invariant and equivariant bases.},
author = {Haggai Maron and Heli Ben-Hamu and Nadav Shamir and Y. Lipman},
arxivid = {1812.09902},
}

@article{a126bcc5380ca588cf5bf8ddace520a71836cc09,
title = {Quantum-based subgraph convolutional neural networks},
year = {2019},
url = {https://www.semanticscholar.org/paper/a126bcc5380ca588cf5bf8ddace520a71836cc09},
abstract = {Abstract This paper proposes a new graph convolutional neural network architecture based on a depth-based representation of graph structure deriving from quantum walks, which we refer to as the quantum-based subgraph convolutional neural network (QS-CNNs). This new architecture captures both the global topological structure and the local connectivity structure within a graph. Specifically, we commence by establishing a family of K-layer expansion subgraphs for each vertex of a graph by quantum walks, which captures the global topological arrangement information for substructures contained within a graph. We then design a set of fixed-size convolution filters over the subgraphs, which helps to characterise multi-scale patterns residing in the data. The idea is to apply convolution filters sliding over the entire set of subgraphs rooted at a vertex to extract the local features analogous to the standard convolution operation on grid data. Experiments on eight graph-structured datasets demonstrate that QS-CNNs architecture is capable of outperforming fourteen state-of-the-art methods for the tasks of node classification and graph classification.},
author = {Zhihong Zhang and Dongdong Chen and Jianjia Wang and Lu Bai and E. Hancock},
doi = {10.1016/J.PATCOG.2018.11.002},
}

@article{4407ee31d3906affc1a7a707ccb3d47219e825d0,
title = {Structured self-attention architecture for graph-level representation learning},
year = {2020},
url = {https://www.semanticscholar.org/paper/4407ee31d3906affc1a7a707ccb3d47219e825d0},
abstract = {Abstract Recently, graph neural networks (GNNs) have shown to be effective in learning representative graph features. However, current pooling-based strategies for graph classification lack efficient utilization of graph representation information in which each node and layer have the same contribution to the output of graph-level representation. In this paper, we develop a novel architecture for extracting an effective graph representation by introducing structured multi-head self-attention in which the attention mechanism consists of three different forms, i.e., node-focused, layer-focused and graph-focused. In order to make full use of the information of graphs, the node-focused self-attention firstly aggregates neighbor node features with a scaled dot-product manner, and then the layer-focused and graph-focused self-attention serve as readout module to measure the importance of different nodes and layers to the model’s output. Moreover, it is able to improve the performance on graph classification tasks by combining these two self-attention mechanisms with base node-level GNNs. The proposed Structured Self-attention Architecture is evaluated on two kinds of graph benchmarks: bioinformatics datasets and social network datasets. Extensive experiments have demonstrated superior performance improvement to existing methods on predictive accuracy.},
author = {Xiaolong Fan and Maoguo Gong and Yu Xie and Fenlong Jiang and Hao Li},
doi = {10.1016/j.patcog.2019.107084},
}

@article{3db5fcb595492dcd64663c00d56f004dfafa689c,
title = {A Fair Comparison of Graph Neural Networks for Graph Classification},
year = {2019},
url = {https://www.semanticscholar.org/paper/3db5fcb595492dcd64663c00d56f004dfafa689c},
abstract = {Experimental reproducibility and replicability are critical topics in machine learning. Authors have often raised concerns about their lack in scientific publications to improve the quality of the field. Recently, the graph representation learning field has attracted the attention of a wide research community, which resulted in a large stream of works. As such, several Graph Neural Network models have been developed to effectively tackle graph classification. However, experimental procedures often lack rigorousness and are hardly reproducible. Motivated by this, we provide an overview of common practices that should be avoided to fairly compare with the state of the art. To counter this troubling trend, we ran more than 47000 experiments in a controlled and uniform framework to re-evaluate five popular models across nine common benchmarks. Moreover, by comparing GNNs with structure-agnostic baselines we provide convincing evidence that, on some datasets, structural information has not been exploited yet. We believe that this work can contribute to the development of the graph learning field, by providing a much needed grounding for rigorous evaluations of graph classification models.},
author = {Federico Errica and Marco Podda and D. Bacciu and A. Micheli},
arxivid = {1912.09893},
}

@article{604d4dcf9d7e1c9a31cd817b29fe2d978fec0151,
title = {MxPool: Multiplex Pooling for Hierarchical Graph Representation Learning},
year = {2020},
url = {https://www.semanticscholar.org/paper/604d4dcf9d7e1c9a31cd817b29fe2d978fec0151},
abstract = {How to utilize deep learning methods for graph classification tasks has attracted considerable research attention in the past few years. Regarding graph classification tasks, the graphs to be classified may have various graph sizes (i.e., different number of nodes and edges) and have various graph properties (e.g., average node degree, diameter, and clustering coefficient). The diverse property of graphs has imposed significant challenges on existing graph learning techniques since diverse graphs have different best-fit hyperparameters. It is difficult to learn graph features from a set of diverse graphs by a unified graph neural network. This motivates us to use a multiplex structure in a diverse way and utilize a priori properties of graphs to guide the learning. In this paper, we propose MxPool, which concurrently uses multiple graph convolution/pooling networks to build a hierarchical learning structure for graph representation learning tasks. Our experiments on numerous graph classification benchmarks show that our MxPool has superiority over other state-of-the-art graph representation learning methods.},
author = {Yanyan Liang and Yanfeng Zhang and Dechao Gao and Qian Xu},
arxivid = {2004.06846},
}

@article{5fb4947831352af6d6231a830a943f0f2069ee8b,
title = {KerGNNs: Interpretable Graph Neural Networks with Graph Kernels},
year = {2022},
url = {https://www.semanticscholar.org/paper/5fb4947831352af6d6231a830a943f0f2069ee8b},
abstract = {Graph kernels are historically the most widely-used technique for graph classification tasks. However, these methods suffer from limited performance because of the hand-crafted combinatorial features of graphs. In recent years, graph neural networks (GNNs) have become the state-of-the-art method in downstream graph-related tasks due to their superior performance. Most GNNs are based on Message Passing Neural Network (MPNN) frameworks. However, recent studies show that MPNNs can not exceed the power of the Weisfeiler-Lehman (WL) algorithm in graph isomorphism test. To address the limitations of existing graph kernel and GNN methods, in this paper, we propose a novel GNN framework, termed Kernel Graph Neural Networks (KerGNNs), which integrates graph kernels into the message passing process of GNNs. Inspired by convolution filters in convolutional neural networks (CNNs), KerGNNs adopt trainable hidden graphs as graph filters which are combined with subgraphs to update node embeddings using graph kernels. In addition, we show that MPNNs can be viewed as special cases of KerGNNs. We apply KerGNNs to multiple graph-related tasks and use cross-validation to make fair comparisons with benchmarks. We show that our method achieves competitive performance compared with existing state-of-the-art methods, demonstrating the potential to increase the representation ability of GNNs. We also show that the trained graph filters in KerGNNs can reveal the local graph structures of the dataset, which significantly improves the model interpretability compared with conventional GNN models.},
author = {Aosong Feng and Chenyu You and Shiqiang Wang and L. Tassiulas},
doi = {10.1609/aaai.v36i6.20615},
arxivid = {2201.00491},
}

@article{9c2550039979df6a85b9781f4255df4be4cf087b,
title = {Unsupervised Hierarchical Graph Representation Learning by Mutual Information Maximization},
year = {2020},
url = {https://www.semanticscholar.org/paper/9c2550039979df6a85b9781f4255df4be4cf087b},
abstract = {Graph representation learning based on graph neural networks (GNNs) can greatly improve the performance of downstream tasks, such as node and graph classification. However, the general GNN models do not aggregate node information in a hierarchical manner, and can miss key higher-order structural features of many graphs. The hierarchical aggregation also enables the graph representations to be explainable. In addition, supervised graph representation learning requires labeled data, which is expensive and error-prone. To address these issues, we present an unsupervised graph representation learning method, Unsupervised Hierarchical Graph Representation (UHGR), which can generate hierarchical representations of graphs. Our method focuses on maximizing mutual information between "local" and high-level "global" representations, which enables us to learn the node embeddings and graph embeddings without any labeled data. To demonstrate the effectiveness of the proposed method, we perform the node and graph classification using the learned node and graph embeddings. The results show that the proposed method achieves comparable results to state-of-the-art supervised methods on several benchmarks. In addition, our visualization of hierarchical representations indicates that our method can capture meaningful and interpretable clusters.},
author = {Fei Ding and Xiaohong Zhang and Justin Sybrandt and Ilya Safro},
arxivid = {2003.08420},
}

@article{ebbf7b57ae70b0ab093d559cc77ffa0f3f6a76ba,
title = {Multi-Channel Graph Neural Networks},
year = {2020},
url = {https://www.semanticscholar.org/paper/ebbf7b57ae70b0ab093d559cc77ffa0f3f6a76ba},
abstract = {The classification of graph-structured data has become increasingly crucial in many disciplines. It has been observed that the implicit or explicit hierarchical community structures preserved in realworld graphs could be useful for downstream classification applications. A straightforward way to leverage the hierarchical structures is to make use of pooling algorithm to cluster nodes into fixed groups, and shrink the input graph layer by layer to learn the pooled graphs. However, the pool shrinking discards graph details to make it hard to distinguish two non-isomorphic graphs, and the fixed clustering ignores the inherent multiple characteristics of nodes. To compensate the shrinking loss and learn the various nodes’ characteristics, we propose the multi-channel graph neural networks (MuchGNN). Motivated by the underlying mechanisms developed in convolutional neural networks, we define the tailored graph convolutions to learn a series of graph channels at each layer, and shrink the graphs hierarchically to encode the pooled structures. Experimental results on real-world datasets demonstrate the superiority of MuchGNN over the state-of-the-art methods.},
author = {Kaixiong Zhou and Qingquan Song and Xiao Huang and D. Zha and Na Zou and Xia Hu},
doi = {10.24963/ijcai.2020/188},
}

@article{6c252187647a437b32b163a295d62b65cda6e0fe,
title = {StructPool: Structured Graph Pooling via Conditional Random Fields},
year = {2020},
url = {https://www.semanticscholar.org/paper/6c252187647a437b32b163a295d62b65cda6e0fe},
abstract = {Learning high-level representations for graphs is of great importance for graph analysis tasks. In addition to graph convolution, graph pooling is an important but less explored research area. In particular, most of existing graph pooling techniques do not consider the graph structural information explicitly. We argue that such information is important and develop a novel graph pooling technique, know as the StructPool, in this work. We consider the graph pooling as a node clustering problem, which requires the learning of a cluster assignment matrix. We propose to formulate it as a structured prediction problem and employ conditional random fields to capture the relationships among assignments of different nodes. We also generalize our method to incorporate graph topological information in designing the Gibbs energy function. Experimental results on multiple datasets demonstrate the effectiveness of our proposed StructPool.},
author = {Hao Yuan and Shuiwang Ji},
}

@article{68f94662dabce3478924aa88decac130a7da5eb0,
title = {A Hyperbolic-to-Hyperbolic Graph Convolutional Network},
year = {2021},
url = {https://www.semanticscholar.org/paper/68f94662dabce3478924aa88decac130a7da5eb0},
abstract = {Hyperbolic graph convolutional networks (GCNs) demonstrate powerful representation ability to model graphs with hierarchical structure. Existing hyperbolic GCNs resort to tangent spaces to realize graph convolution on hyperbolic manifolds, which is inferior because tangent space is only a local approximation of a manifold. In this paper, we propose a hyperbolic-to-hyperbolic graph convolutional network (H2H-GCN) that directly works on hyperbolic manifolds. Specifically, we developed a manifold-preserving graph convolution that consists of a hyperbolic feature transformation and a hyperbolic neighborhood aggregation. The hyperbolic feature transformation works as linear transformation on hyperbolic manifolds. It ensures the transformed node representations still lie on the hyperbolic manifold by imposing the orthogonal constraint on the transformation sub-matrix. The hyperbolic neighborhood aggregation updates each node representation via the Einstein midpoint. The H2H-GCN avoids the distortion caused by tangent space approximations and keeps the global hyperbolic structure. Extensive experiments show that the H2H-GCN achieves substantial improvements on the link prediction, node classification, and graph classification tasks.},
author = {Jindou Dai and Yuwei Wu and Zhi Gao and Yunde Jia},
doi = {10.1109/CVPR46437.2021.00022},
arxivid = {2104.06942},
}

@article{12ebe7a1a30bed87b196f1f8e4b1be18e850bdc0,
title = {Second-Order Pooling for Graph Neural Networks},
year = {2020},
url = {https://www.semanticscholar.org/paper/12ebe7a1a30bed87b196f1f8e4b1be18e850bdc0},
abstract = {Graph neural networks have achieved great success in learning node representations for graph tasks such as node classification and link prediction. Graph representation learning requires graph pooling to obtain graph representations from node representations. It is challenging to develop graph pooling methods due to the variable sizes and isomorphic structures of graphs. In this work, we propose to use second-order pooling as graph pooling, which naturally solves the above challenges. In addition, compared to existing graph pooling methods, second-order pooling is able to use information from all nodes and collect second-order statistics, making it more powerful. We show that direct use of second-order pooling with graph neural networks leads to practical problems. To overcome these problems, we propose two novel global graph pooling methods based on second-order pooling; namely, bilinear mapping and attentional second-order pooling. In addition, we extend attentional second-order pooling to hierarchical graph pooling for more flexible use in GNNs. We perform thorough experiments on graph classification tasks to demonstrate the effectiveness and superiority of our proposed methods. Experimental results show that our methods improve the performance significantly and consistently.},
author = {Zhengyang Wang and Shuiwang Ji},
doi = {10.1109/TPAMI.2020.2999032},
pmid = {32750778},
arxivid = {2007.10467},
}

@article{d1b9543ac77198e56e36157e2bada7a8eadd244b,
title = {Graph Neural Networks with Parallel Neighborhood Aggregations for Graph Classification},
year = {2021},
url = {https://www.semanticscholar.org/paper/d1b9543ac77198e56e36157e2bada7a8eadd244b},
abstract = {We focus on graph classification using a graph neural network (GNN) model that precomputes the node features using a bank of neighborhood aggregation graph operators arranged in parallel. These GNN models have a natural advantage of reduced training and inference time due to the precomputations but are also fundamentally different from popular GNN variants that update node features through a sequential neighborhood aggregation procedure during training. We provide theoretical conditions under which a generic GNN model with parallel neighborhood aggregations (PA-GNNs, in short) are provably as powerful as the well-known Weisfeiler-Lehman (WL) graph isomorphism test in discriminating non-isomorphic graphs. Although PA-GNN models do not have an apparent relationship with the WL test, we show that the graph embeddings obtained from these two methods are injectively related. We then propose a specialized PA-GNN model, called SPIN, which obeys the developed conditions. We demonstrate via numerical experiments that the developed model achieves state-of-the-art performance on many diverse real-world datasets while maintaining the discriminative power of the WL test and the computational advantage of preprocessing graphs before the training process.},
author = {Siddhant Doshi and S. P. Chepuri},
arxivid = {2111.11482},
}

@article{1da4770696ab4ad7bc32dc4d92425987eac0fb67,
title = {AttPool: Towards Hierarchical Feature Representation in Graph Convolutional Networks via Attention Mechanism},
year = {2019},
url = {https://www.semanticscholar.org/paper/1da4770696ab4ad7bc32dc4d92425987eac0fb67},
abstract = {Graph convolutional networks (GCNs) are potentially short of the ability to learn hierarchical representation for graph embedding, which holds them back in the graph classification task. Here, we propose AttPool, which is a novel graph pooling module based on attention mechanism, to remedy the problem. It is able to select nodes that are significant for graph representation adaptively, and generate hierarchical features via aggregating the attention-weighted information in nodes. Additionally, we devise a hierarchical prediction architecture to sufficiently leverage the hierarchical representation and facilitate the model learning. The AttPool module together with the entire training structure can be integrated into existing GCNs, and is trained in an end-to-end fashion conveniently. The experimental results on several graph-classification benchmark datasets with various scales demonstrate the effectiveness of our method.},
author = {Jingjia Huang and Zhangheng Li and Nannan Li and Shan Liu and Ge Li},
doi = {10.1109/ICCV.2019.00658},
}

@article{04a609364a1a9fc22aaef68a47bb85f22cf87648,
title = {Graph Convolutional Neural Networks with Node Transition Probability-based Message Passing and DropNode Regularization},
year = {2020},
url = {https://www.semanticscholar.org/paper/04a609364a1a9fc22aaef68a47bb85f22cf87648},
abstract = {null},
author = {T. Do and Duc Minh Nguyen and Giannis Bekoulis and A. Munteanu and N. Deligiannis},
doi = {10.1016/J.ESWA.2021.114711},
arxivid = {2008.12578},
}

@article{f1964cc1b343f907462cd7fbfaf88c2679d61f59,
title = {Graph U-Nets},
year = {2019},
url = {https://www.semanticscholar.org/paper/f1964cc1b343f907462cd7fbfaf88c2679d61f59},
abstract = {We consider the problem of representation learning for graph data. Given images are special cases of graphs with nodes lie on 2D lattices, graph embedding tasks have a natural correspondence with image pixel-wise prediction tasks such as segmentation. While encoder-decoder architectures like U-Nets have been successfully applied to image pixel-wise prediction tasks, similar methods are lacking for graph data. This is because pooling and up-sampling operations are not natural on graph data. To address these challenges, we propose novel graph pooling and unpooling operations. The gPool layer adaptively selects some nodes to form a smaller graph based on their scalar projection values. We further propose the gUnpool layer as the inverse operation of the gPool layer. Based on our proposed methods, we develop an encoder-decoder model, known as the graph U-Nets. Experimental results on node classification and graph classification tasks demonstrate that our methods achieve consistently better performance than previous models. Along this direction, we extend our methods by integrating attention mechanisms. Based on attention operators, we proposed attention-based pooling and unpooling layers, which can better capture graph topology information. The empirical results on graph classification tasks demonstrate the promising capability of our methods.},
author = {Hongyang Gao and Shuiwang Ji},
doi = {10.1109/TPAMI.2021.3081010},
pmid = {33999813},
arxivid = {1905.05178},
}

@article{42ee424dd1aa07d8579b912bc8dc10719b47c643,
title = {From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked Transformers},
year = {2021},
url = {https://www.semanticscholar.org/paper/42ee424dd1aa07d8579b912bc8dc10719b47c643},
abstract = {In this paper we provide, to the best of our knowledge, the ﬁrst comprehensive approach for incorporating various masking mechanisms into Transformers architectures in a scalable way. We show that recent results on linear causal attention (Choromanski et al., 2021) and log-linear RPE-attention (Luo et al., 2021) are special cases of this general mechanism. However by casting the problem as a topological (graph-based) modulation of unmasked attention, we obtain several results un-known before, including efﬁcient d -dimensional RPE-masking and graph-kernel masking. We leverage many mathematical techniques ranging from spectral analysis through dynamic programming and random walks to new algorithms for solving Markov processes on graphs. We provide a corresponding empirical evaluation.},
author = {K. Choromanski and Han Lin and Haoxian Chen and Tianyi Zhang and Arijit Sehanobish and Valerii Likhosherstov and Jack Parker-Holder and Tamás Sarlós and Adrian Weller and Thomas Weingarten},
arxivid = {2107.07999},
}

@article{f504a853a887ce0aac569d6c49c095ca5f551631,
title = {Graph Convolutional Neural Networks via Motif-based Attention},
year = {2018},
url = {https://www.semanticscholar.org/paper/f504a853a887ce0aac569d6c49c095ca5f551631},
abstract = {Many real-world problems can be represented as graph-based learning problems. In this paper, we propose a novel framework for learning spatial and attentional convolution neural networks on arbitrary graphs. Different from previous convolutional neural networks on graphs, we first design a motif-matching guided subgraph normalization method to capture neighborhood information. Then we implement subgraph-level self-attentional layers to learn different importances from different subgraphs to solve graph classification problems. Analogous to image-based attentional convolution networks that operate on locally connected and weighted regions of the input, we also extend graph normalization from one-dimensional node sequence to two-dimensional node grid by leveraging motif-matching, and design self-attentional layers without requiring any kinds of cost depending on prior knowledge of the graph structure. Our results on both bioinformatics and social network datasets show that we can significantly improve graph classification benchmarks over traditional graph kernel and existing deep models.},
author = {Hao Peng and Jianxin Li and Qiran Gong and Yuanxing Ning and Lihong Wang},
arxivid = {1811.08270},
}

@article{0d4870527aab05caebdc9226ce52bce2dc9f3438,
title = {Non-IID Graph Neural Networks},
year = {2020},
url = {https://www.semanticscholar.org/paper/0d4870527aab05caebdc9226ce52bce2dc9f3438},
abstract = {Graph classification is an important task on graph-structured data with many real-world applications. The goal of graph classification task is to train a classifier using a set of training graphs. Recently, Graph Neural Networks (GNNs) have greatly advanced the task of graph classification. When building a GNN model for graph classification, the graphs in the training set are usually assumed to be identically distributed. However, in many real-world applications, graphs in the same dataset could have dramatically different structures, which indicates that these graphs are likely non-identically distributed. Therefore, in this paper, we aim to develop graph neural networks for graphs that are not non-identically distributed. Specifically, we propose a general non-IID graph neural network framework, i.e., Non-IID-GNN. Given a graph, Non-IID-GNN can adapt any existing graph neural network model to generate a sample-specific model for this graph. Comprehensive experiments on various graph classification benchmarks demonstrate the effectiveness of the proposed framework. We will release the code of the proposed framework upon the acceptance of the paper.},
author = {Yiqi Wang and Yao Ma and C. Aggarwal and Jiliang Tang},
}

@article{5dcded563bd70b3f000ab8ca2c8b2400a3adc046,
title = {SOM-based aggregation for graph convolutional neural networks},
year = {2020},
url = {https://www.semanticscholar.org/paper/5dcded563bd70b3f000ab8ca2c8b2400a3adc046},
abstract = {Graph property prediction is becoming more and more popular due to the increasing availability of scientific and social data naturally represented in a graph form. Because of that, many researchers are focusing on the development of improved graph neural network models. One of the main components of a graph neural network is the aggregation operator, needed to generate a graph-level representation from a set of node-level embeddings. The aggregation operator is critical since it should, in principle, provide a representation of the graph that is isomorphism invariant, i.e. the graph representation should be a function of graph nodes treated as a set. DeepSets (in: Advances in neural information processing systems, pp 3391–3401, 2017) provides a framework to construct a set-aggregation operator with universal approximation properties. In this paper, we propose a DeepSets aggregation operator, based on Self-Organizing Maps (SOM), to transform a set of node-level representations into a single graph-level one. The adoption of SOMs allows to compute node representations that embed the information about their mutual similarity. Experimental results on several real-world datasets show that our proposed approach achieves improved predictive performance compared to the commonly adopted sum aggregation and many state-of-the-art graph neural network architectures in the literature.},
author = {Luca Pasa and Nicolò Navarin and A. Sperduti},
doi = {10.1007/s00521-020-05484-4},
}

@article{442db0ef2d0ff9d45aeed77010315396c34960aa,
title = {ASAP: Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations},
year = {2019},
url = {https://www.semanticscholar.org/paper/442db0ef2d0ff9d45aeed77010315396c34960aa},
abstract = {Graph Neural Networks (GNN) have been shown to work effectively for modeling graph structured data to solve tasks such as node classification, link prediction and graph classification. There has been some recent progress in defining the notion of pooling in graphs whereby the model tries to generate a graph level representation by downsampling and summarizing the information present in the nodes. Existing pooling methods either fail to effectively capture the graph substructure or do not easily scale to large graphs. In this work, we propose ASAP (Adaptive Structure Aware Pooling), a sparse and differentiable pooling method that addresses the limitations of previous graph pooling architectures. ASAP utilizes a novel self-attention network along with a modified GNN formulation to capture the importance of each node in a given graph. It also learns a sparse soft cluster assignment for nodes at each layer to effectively pool the subgraphs to form the pooled graph. Through extensive experiments on multiple datasets and theoretical analysis, we motivate our choice of the components used in ASAP. Our experimental results show that combining existing GNN architectures with ASAP leads to state-of-the-art results on multiple graph classification benchmarks. ASAP has an average improvement of 4%, compared to current sparse hierarchical state-of-the-art method. We make the source code of ASAP available to encourage reproducible research 1.},
author = {Ekagra Ranjan and Soumya Sanyal and P. Talukdar},
doi = {10.1609/AAAI.V34I04.5997},
arxivid = {1911.07979},
}

@article{303bb47635fdebfed91c1390d623c287b606b18e,
title = {A Non-Negative Factorization approach to node pooling in Graph Convolutional Neural Networks},
year = {2019},
url = {https://www.semanticscholar.org/paper/303bb47635fdebfed91c1390d623c287b606b18e},
abstract = {The paper discusses a pooling mechanism to induce subsampling in graph structured data and introduces it as a component of a graph convolutional neural network. The pooling mechanism builds on the Non-Negative Matrix Factorization (NMF) of a matrix representing node adjacency and node similarity as adaptively obtained through the vertices embedding learned by the model. Such mechanism is applied to obtain an incrementally coarser graph where nodes are adaptively pooled into communities based on the outcomes of the non-negative factorization. The empirical analysis on graph classification benchmarks shows how such coarsening process yields significant improvements in the predictive performance of the model with respect to its non-pooled counterpart.},
author = {D. Bacciu and Luigi Di Sotto},
doi = {10.1007/978-3-030-35166-3_21},
arxivid = {1909.03287},
}

@article{3641cba49fce0c02c764c163bc45d5500307c47f,
title = {Interpreting Deep Neural Networks Beyond Attribution Methods: Quantifying Global Importance of Genomic Features},
year = {2020},
url = {https://www.semanticscholar.org/paper/3641cba49fce0c02c764c163bc45d5500307c47f},
abstract = {Despite deep neural networks (DNNs) having found great success at improving performance on various prediction tasks in computational genomics, it remains difficult to understand why they make any given prediction. In genomics, the main approaches to interpret a high-performing DNN are to visualize learned representations via weight visualizations and attribution methods. While these methods can be informative, each has strong limitations. For instance, attribution methods only uncover the independent contribution of single nucleotide variants in a given sequence. Here we discuss and argue for global importance analysis which can quantify population-level importance of putative features and their interactions learned by a DNN. We highlight recent work that has benefited from this interpretability approach and then discuss connections between global importance analysis and causality.},
author = {Peter K. Koo and Matthew Ploenzke},
doi = {10.1101/2020.02.19.956896},
}

@article{01b5795840b41f80b7e121f8208e8c202ec8b54c,
title = {Selene: a PyTorch-based deep learning library for sequence data},
year = {2019},
url = {https://www.semanticscholar.org/paper/01b5795840b41f80b7e121f8208e8c202ec8b54c},
abstract = {To enable the application of deep learning in biology, we present Selene (https://selene.flatironinstitute.org/), a PyTorch-based deep learning library for fast and easy development, training, and application of deep learning model architectures for any biological sequence data. We demonstrate on DNA sequences how Selene allows researchers to easily train a published architecture on new data, develop and evaluate a new architecture, and use a trained model to answer biological questions of interest.Selene is a deep learning library that enables the expansion of existing deep learning models to new data, the development of new model architectures, and the evaluation of these new models on biological sequence data.},
author = {Kathleen M. Chen and Evan M. Cofer and Jian Zhou and O. Troyanskaya},
doi = {10.1038/s41592-019-0360-8},
pmid = {30923381},
}

@article{5f4fae84e1f858697260450b6daaa79da42b633d,
title = {Selene: a PyTorch-based deep learning library for sequence-level data},
year = {2018},
url = {https://www.semanticscholar.org/paper/5f4fae84e1f858697260450b6daaa79da42b633d},
abstract = {To enable the application of deep learning in biology, we present Selene (https://selene.flatironinstitute.org/), a PyTorch-based deep learning library for fast and easy development, training, and application of deep learning model architectures for any biological sequences. We demonstrate how Selene allows researchers to easily train a published architecture on new data, develop and evaluate a new architecture, and use a trained model to answer biological questions of interest.},
author = {Kathleen M. Chen and Evan M. Cofer and Jian Zhou and O. Troyanskaya},
doi = {10.1101/438291},
}

@article{884a94b5e7da401bc89ad0ea5e7bddea185079be,
title = {Deep learning for genomics using Janggu},
year = {2020},
url = {https://www.semanticscholar.org/paper/884a94b5e7da401bc89ad0ea5e7bddea185079be},
abstract = {In recent years, numerous applications have demonstrated the potential of deep learning for an improved understanding of biological processes. However, most deep learning tools developed so far are designed to address a specific question on a fixed dataset and/or by a fixed model architecture. Here we present Janggu, a python library facilitates deep learning for genomics applications, aiming to ease data acquisition and model evaluation. Among its key features are special dataset objects, which form a unified and flexible data acquisition and pre-processing framework for genomics data that enables streamlining of future research applications through reusable components. Through a numpy-like interface, these dataset objects are directly compatible with popular deep learning libraries, including keras or pytorch. Janggu offers the possibility to visualize predictions as genomic tracks or by exporting them to the bigWig format as well as utilities for keras-based models. We illustrate the functionality of Janggu on several deep learning genomics applications. First, we evaluate different model topologies for the task of predicting binding sites for the transcription factor JunD. Second, we demonstrate the framework on published models for predicting chromatin effects. Third, we show that promoter usage measured by CAGE can be predicted using DNase hypersensitivity, histone modifications and DNA sequence features. We improve the performance of these models due to a novel feature in Janggu that allows us to include high-order sequence features. We believe that Janggu will help to significantly reduce repetitive programming overhead for deep learning applications in genomics, and will enable computational biologists to rapidly assess biological hypotheses. Deep learning is becoming a popular approach for understanding biological processes but can be hard to adapt to new questions. Here, the authors develop Janggu, a python library that aims to ease data acquisition and model evaluation and facilitate deep learning applications in genomics.},
author = {W. Kopp and Remo Monti and Annalaura Tamburrini and U. Ohler and A. Akalin},
doi = {10.1038/s41467-020-17155-y},
pmid = {32661261},
}

@article{e1039fcb6cfc0576c1a20679b574032a719fd9cf,
title = {Deep Neural Networks for Epistatic Sequence Analysis.},
year = {2021},
url = {https://www.semanticscholar.org/paper/e1039fcb6cfc0576c1a20679b574032a719fd9cf},
abstract = {We report a step-by-step protocol to use pysster, a TensorFlow-based package for building deep neural networks on a broad range of epistatic sequences such as DNA, RNA, or annotated secondary structure sequences. Pysster provides users comprehensive supports for developing, training, and evaluating the self-defined deep neural networks on sequence data. Moreover, pysster allows users to easily visualize the resulting perditions, which is helpful to uncover the "black box" of deep neural networks. Here, we describe a step-by-step application of pysster to classify the RNA A-to-I editing regions and interpret the model predictions. To further demonstrate the generalizability of pysster, we utilized it to build and evaluated a new deep neural network on an artificial epistatic sequence dataset.},
author = {Jiecong Lin},
doi = {10.1007/978-1-0716-0947-7_17},
pmid = {33733362},
}
